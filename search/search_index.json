{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LOTD - Lord Of The Datasets LOTD (Lord Of The Datasets) is an efficient and flexible NLP dataset preprocessing library designed to simplify working with large-scale text datasets. Its goal is to make dataset preparation, filtering, tokenization, and batching seamless for research. Motivation Preparing NLP datasets for model training can be tedious and error-prone. Tasks such as: Tokenizing text or chat dialogs Applying templates for instruction tuning Filtering sequences by length Padding and batching for PyTorch often require custom scripts that are hard to maintain and reuse. LOTD aims to solve this by providing reusable building blocks and ready-to-use utilities , so developers and researchers can focus on modeling instead of boilerplate preprocessing. Features Overview LOTD provides: Flexible tokenizers for text and chat datasets, including support for system, user, and assistant roles. Collators and filters that handle padding, batching, and sequence length constraints efficiently. Dataset utilities for splitting, caching, and creating PyTorch DataLoaders from HuggingFace datasets. Prebuilt loaders for popular datasets such as Alpaca, ready for instruction fine-tuning. LOTD is designed to reduce boilerplate , improve reproducibility , and make dataset preprocessing more intuitive , so you can spend more time experimenting with models and less time writing tedious preprocessing code.","title":"Home"},{"location":"#lotd-lord-of-the-datasets","text":"LOTD (Lord Of The Datasets) is an efficient and flexible NLP dataset preprocessing library designed to simplify working with large-scale text datasets. Its goal is to make dataset preparation, filtering, tokenization, and batching seamless for research.","title":"LOTD - Lord Of The Datasets"},{"location":"#motivation","text":"Preparing NLP datasets for model training can be tedious and error-prone. Tasks such as: Tokenizing text or chat dialogs Applying templates for instruction tuning Filtering sequences by length Padding and batching for PyTorch often require custom scripts that are hard to maintain and reuse. LOTD aims to solve this by providing reusable building blocks and ready-to-use utilities , so developers and researchers can focus on modeling instead of boilerplate preprocessing.","title":"Motivation"},{"location":"#features-overview","text":"LOTD provides: Flexible tokenizers for text and chat datasets, including support for system, user, and assistant roles. Collators and filters that handle padding, batching, and sequence length constraints efficiently. Dataset utilities for splitting, caching, and creating PyTorch DataLoaders from HuggingFace datasets. Prebuilt loaders for popular datasets such as Alpaca, ready for instruction fine-tuning. LOTD is designed to reduce boilerplate , improve reproducibility , and make dataset preprocessing more intuitive , so you can spend more time experimenting with models and less time writing tedious preprocessing code.","title":"Features Overview"},{"location":"collators/","text":"Dataset collators Callable objects passed to pytorch dataloaders to prepare process samples in one batch. lotd.PadCollator Collator which converts inputs into pytorch tensors and applies padding. __call__ __call__ ( batch : List [ Dict [ str , Any ]]) -> Dict [ str , torch . Tensor ] Collates inputs. Parameters: batch ( List [ Dict [ str , Any ]] ) \u2013 list of dicts where each key is a feature. Returns: Dict [ str , Tensor ] \u2013 a dict of padded tensors of form {\"input_ids\": ..., \"attention_mask\": ..., \"prompt_mask\": ...} . __init__ __init__ ( pad_id : int , pre : Union [ Callable , None ] = None , post : Union [ Callable , None ] = None , padding_side : Literal [ 'right' , 'left' ] = 'right' ) -> None Initializes the collator. Parameters: pad_id ( int ) \u2013 pad token id used for padding. pre ( Union [ Callable , None] , default: None ) \u2013 callable that accepts list of dicts where every key is a dataset feature, modifies it and returns it back. post ( Union [ Callable , None] , default: None ) \u2013 callable that accepts a dict of tensors of a form {\"input_ids\": ..., \"attention_mask\": ..., \"prompt_mask\": ...} , modifies it and returns it back. padding_side ( Literal ['right', 'left'] , default: 'right' ) \u2013 can be right or left.","title":"Collators"},{"location":"collators/#dataset-collators","text":"Callable objects passed to pytorch dataloaders to prepare process samples in one batch.","title":"Dataset collators"},{"location":"collators/#lotd.PadCollator","text":"Collator which converts inputs into pytorch tensors and applies padding.","title":"PadCollator"},{"location":"collators/#lotd.PadCollator.__call__","text":"__call__ ( batch : List [ Dict [ str , Any ]]) -> Dict [ str , torch . Tensor ] Collates inputs. Parameters: batch ( List [ Dict [ str , Any ]] ) \u2013 list of dicts where each key is a feature. Returns: Dict [ str , Tensor ] \u2013 a dict of padded tensors of form {\"input_ids\": ..., \"attention_mask\": ..., \"prompt_mask\": ...} .","title":"__call__"},{"location":"collators/#lotd.PadCollator.__init__","text":"__init__ ( pad_id : int , pre : Union [ Callable , None ] = None , post : Union [ Callable , None ] = None , padding_side : Literal [ 'right' , 'left' ] = 'right' ) -> None Initializes the collator. Parameters: pad_id ( int ) \u2013 pad token id used for padding. pre ( Union [ Callable , None] , default: None ) \u2013 callable that accepts list of dicts where every key is a dataset feature, modifies it and returns it back. post ( Union [ Callable , None] , default: None ) \u2013 callable that accepts a dict of tensors of a form {\"input_ids\": ..., \"attention_mask\": ..., \"prompt_mask\": ...} , modifies it and returns it back. padding_side ( Literal ['right', 'left'] , default: 'right' ) \u2013 can be right or left.","title":"__init__"},{"location":"datasets/","text":"Datasets Ready-to-use pipelines for some popular datasets lotd.datasets.alpaca alpaca ( tokenizer : Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ], cache_path : Union [ str , None ] = None , max_length : int = 512 , batch_size : int = 16 , seed : int = 42 , pre : Union [ Callable , None ] = None , post : Union [ Callable , None ] = None ) -> Tuple [ DataLoader , DataLoader , DataLoader ] Downloads and Pre-processes Alpaca dataset for instruction fine-tuning. Parameters: tokenizer ( Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ] ) \u2013 transformers tokenizer with chat_template parameter set. cache_path ( Union [ str , None] , default: None ) \u2013 path to load/save dataset. max_length ( int , default: 512 ) \u2013 maximum sequence length for filter. batch_size ( int , default: 16 ) \u2013 dataloaders batch size. seed ( int , default: 42 ) \u2013 splitting and shuffling seed. pre ( Union [ Callable , None] , default: None ) \u2013 see PadCollator . post ( Union [ Callable , None] , default: None ) \u2013 see PadCollator . Returns: Tuple [ DataLoader , DataLoader , DataLoader ] \u2013 a tuple of train, validation and test dataloaders.","title":"Datasets"},{"location":"datasets/#datasets","text":"Ready-to-use pipelines for some popular datasets","title":"Datasets"},{"location":"datasets/#lotd.datasets.alpaca","text":"alpaca ( tokenizer : Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ], cache_path : Union [ str , None ] = None , max_length : int = 512 , batch_size : int = 16 , seed : int = 42 , pre : Union [ Callable , None ] = None , post : Union [ Callable , None ] = None ) -> Tuple [ DataLoader , DataLoader , DataLoader ] Downloads and Pre-processes Alpaca dataset for instruction fine-tuning. Parameters: tokenizer ( Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ] ) \u2013 transformers tokenizer with chat_template parameter set. cache_path ( Union [ str , None] , default: None ) \u2013 path to load/save dataset. max_length ( int , default: 512 ) \u2013 maximum sequence length for filter. batch_size ( int , default: 16 ) \u2013 dataloaders batch size. seed ( int , default: 42 ) \u2013 splitting and shuffling seed. pre ( Union [ Callable , None] , default: None ) \u2013 see PadCollator . post ( Union [ Callable , None] , default: None ) \u2013 see PadCollator . Returns: Tuple [ DataLoader , DataLoader , DataLoader ] \u2013 a tuple of train, validation and test dataloaders.","title":"alpaca"},{"location":"overview/","text":"LOTD - Lord Of The Datasets Full API overview. lotd LOTD - Lord of the Datasets Efficient NLP dataset preprocessing library. ChatTokenizer Applies chat template and tokenizes messages in a dataset. __call__ __call__ ( prompts : Union [ List [ str ], List [ List [ str ]]], responses : Union [ List [ str ], List [ List [ str ]]] = [], system : Union [ List [ str ], None ] = None ) -> Dict [ str , List [ List [ int ]]] Tokenizes chats. Parameters: prompts ( Union [ List [ str ], List [ List [ str ]]] ) \u2013 a list where each item is either a single prompt or multiple prompts in a dialog. responses ( Union [ List [ str ], List [ List [ str ]]] , default: [] ) \u2013 a list where each item is either a single response or multiple responses in a dialog. Returns: Dict [ str , List [ List [ int ]]] \u2013 dict with input_ids and prompt_mask for all text in a batch. __init__ __init__ ( tokenizer : Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ], max_length : Union [ int , None ] = None ) -> None Initializes the dataset chat tokenizer. Parameters: tokenizer ( Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ] ) \u2013 pre-trained transformers tokenizer. max_length ( optional , default: None ) \u2013 maximum sequence length after truncation. LengthFilter Filters datasets by max and min length of input_ids. __call__ __call__ ( input_ids : List [ List [ int ]]) -> List [ bool ] Processes a batch of input_ids. __init__ __init__ ( min_length : int = 0 , max_length : Union [ int , None ] = None ) -> None Initializes the collator. Parameters: min_length ( int , default: 0 ) \u2013 lowest number of tokens. max_length ( Union [ int , None] , default: None ) \u2013 highest number of tokens. PadCollator Collator which converts inputs into pytorch tensors and applies padding. __call__ __call__ ( batch : List [ Dict [ str , Any ]]) -> Dict [ str , torch . Tensor ] Collates inputs. Parameters: batch ( List [ Dict [ str , Any ]] ) \u2013 list of dicts where each key is a feature. Returns: Dict [ str , Tensor ] \u2013 a dict of padded tensors of form {\"input_ids\": ..., \"attention_mask\": ..., \"prompt_mask\": ...} . __init__ __init__ ( pad_id : int , pre : Union [ Callable , None ] = None , post : Union [ Callable , None ] = None , padding_side : Literal [ 'right' , 'left' ] = 'right' ) -> None Initializes the collator. Parameters: pad_id ( int ) \u2013 pad token id used for padding. pre ( Union [ Callable , None] , default: None ) \u2013 callable that accepts list of dicts where every key is a dataset feature, modifies it and returns it back. post ( Union [ Callable , None] , default: None ) \u2013 callable that accepts a dict of tensors of a form {\"input_ids\": ..., \"attention_mask\": ..., \"prompt_mask\": ...} , modifies it and returns it back. padding_side ( Literal ['right', 'left'] , default: 'right' ) \u2013 can be right or left. TextTokenizer Applies template and tokenizes texts in a dataset. __call__ __call__ ( texts : List [ str ]) -> Dict [ str , List [ List [ int ]]] Tokenizes a batch of text samples. Parameters: texts ( List [ str ] ) \u2013 list of strings. Returns: Dict [ str , List [ List [ int ]]] \u2013 dict with input_ids and prompt_mask for all text in a batch. __init__ __init__ ( tokenizer : Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ], template : str = '[CLS]{{text}}[SEP]' , max_length : Union [ int , None ] = None ) -> None Initializes the dataset tokenizer. Parameters: tokenizer ( Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ] ) \u2013 pre-trained transformers tokenizer. template ( str , default: '[CLS]{{text}}[SEP]' ) \u2013 jinja2 template containing {{text}} keyword. max_length ( optional , default: None ) \u2013 maximum sequence length after truncation. format_chat format_chat ( prompts : Union [ str , List [ str ]] = [], responses : Union [ str , List [ str ]] = [], system : Union [ str , None ] = None ) -> List [ Dict [ str , str ]] Format list of prompts, responses and system messages into a chat. Parameters: prompts ( Union [ str , List [ str ]] , default: [] ) \u2013 a single prompt or a list of prompts provided by a user in a dialog. responses ( Union [ str , List [ str ]] , default: [] ) \u2013 a single response or a list of responses producrd by a model in a dialog. system ( Union [ str , None] , default: None ) \u2013 system prompt used for this chat. Returns: List [ Dict [ str , str ]] \u2013 a list of messages of form [{\"role\": \"...\", \"content\": \"...\"}, ...] . generate_chat_template generate_chat_template ( last_only : bool = False , wrapper : str = '{{template}}' , system_tags : Tuple [ str , str ] = ( '<|system|>' , '<|end|>' ), user_tags : Tuple [ str , str ] = ( '<|user|>' , '<|end|>' ), assistant_tags : Tuple [ str , str ] = ( '<|assistant|>' , '<|end|>' )) -> str Generate chat template for instruction tuning. Parameters: last_only ( bool , default: False ) \u2013 only enable generation for last response. If true, assistant_masks ignores previous responses. Default: False. wrapper ( str , default: '{{template}}' ) \u2013 a wrapper for adding text before and after the template. Should contain {{template}} . system_tags ( Tuple [ str , str ] , default: ('<|system|>', '<|end|>') ) \u2013 a pair of tags to mark the beginning and the end of a system role in a dialog. user_tags ( Tuple [ str , str ] , default: ('<|user|>', '<|end|>') ) \u2013 see system_tags . assistant_tags ( Tuple [ str , str ] , default: ('<|assistant|>', '<|end|>') ) \u2013 see system_tags . Returns: str \u2013 a jinja2 template string. get_loaders get_loaders ( dataset : Dataset , collate_fn : Callable = lambda x : x , batch_size : int = 16 , train_size : float = 0.8 , val_size : float = 0.1 , num_workers : int = 15 , seed : int = 42 ) -> Tuple [ DataLoader , DataLoader , DataLoader ] Shortcut to generate pytorch dataloaders (train/val/test) from hf dataset. Parameters: dataset ( Dataset ) \u2013 HF dataset. collate_fn ( Callable , default: lambda x: x ) \u2013 function used for dataset collation. batch_size ( int , default: 16 ) \u2013 batch_size for dataloaders. train_size ( float , default: 0.8 ) \u2013 train split size. see split_dataset . val_size ( float , default: 0.1 ) \u2013 validation split size. see split_dataset . num_workers ( int , default: 15 ) \u2013 number of pytorch dataloader workers. seed ( int , default: 42 ) \u2013 random seed used for splitting. Returns: Tuple [ DataLoader , DataLoader , DataLoader ] \u2013 a tuple with train, validation and test pytorch dataloaders. Splits dataset and assigns collators automatically. load_cached load_cached ( cache_path : str , process_fn : Callable ) -> Dataset Try loading processed dataset from cache. Processes dataset and saves it to cache if pre-cached dataset is not found. Parameters: cache_path ( str ) \u2013 path to load/save dataset. process_fn ( Callable ) \u2013 function that will return a new processed dataset if cache is not found. Returns: Dataset \u2013 a pre-processed HF dataset. split_dataset split_dataset ( dataset : Dataset , train_size : float = 0.8 , val_size : float = 0.1 , seed : int = 42 ) -> Tuple [ Dataset , Dataset , Dataset ] Split HF dataset into train, validation and test. Parameters: dataset ( Dataset ) \u2013 HF dataset. train_size ( float , default: 0.8 ) \u2013 train ratio from 0 to 1. val_size ( float , default: 0.1 ) \u2013 validation ratio from 0 to 1. seed ( int , default: 42 ) \u2013 seed used for splitting. Returns: Tuple [ Dataset , Dataset , Dataset ] \u2013 a tuple of 3 datasets for train, validation and test. train_size and val_size are taken from total and their sum should not be more than 1. Test size would be equal to 1 - train_size - val_size. Datasets lotd.datasets alpaca alpaca ( tokenizer : Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ], cache_path : Union [ str , None ] = None , max_length : int = 512 , batch_size : int = 16 , seed : int = 42 , pre : Union [ Callable , None ] = None , post : Union [ Callable , None ] = None ) -> Tuple [ DataLoader , DataLoader , DataLoader ] Downloads and Pre-processes Alpaca dataset for instruction fine-tuning. Parameters: tokenizer ( Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ] ) \u2013 transformers tokenizer with chat_template parameter set. cache_path ( Union [ str , None] , default: None ) \u2013 path to load/save dataset. max_length ( int , default: 512 ) \u2013 maximum sequence length for filter. batch_size ( int , default: 16 ) \u2013 dataloaders batch size. seed ( int , default: 42 ) \u2013 splitting and shuffling seed. pre ( Union [ Callable , None] , default: None ) \u2013 see PadCollator . post ( Union [ Callable , None] , default: None ) \u2013 see PadCollator . Returns: Tuple [ DataLoader , DataLoader , DataLoader ] \u2013 a tuple of train, validation and test dataloaders.","title":"Overview"},{"location":"overview/#lotd-lord-of-the-datasets","text":"Full API overview.","title":"LOTD - Lord Of The Datasets"},{"location":"overview/#lotd","text":"LOTD - Lord of the Datasets Efficient NLP dataset preprocessing library.","title":"lotd"},{"location":"overview/#lotd.ChatTokenizer","text":"Applies chat template and tokenizes messages in a dataset.","title":"ChatTokenizer"},{"location":"overview/#lotd.ChatTokenizer.__call__","text":"__call__ ( prompts : Union [ List [ str ], List [ List [ str ]]], responses : Union [ List [ str ], List [ List [ str ]]] = [], system : Union [ List [ str ], None ] = None ) -> Dict [ str , List [ List [ int ]]] Tokenizes chats. Parameters: prompts ( Union [ List [ str ], List [ List [ str ]]] ) \u2013 a list where each item is either a single prompt or multiple prompts in a dialog. responses ( Union [ List [ str ], List [ List [ str ]]] , default: [] ) \u2013 a list where each item is either a single response or multiple responses in a dialog. Returns: Dict [ str , List [ List [ int ]]] \u2013 dict with input_ids and prompt_mask for all text in a batch.","title":"__call__"},{"location":"overview/#lotd.ChatTokenizer.__init__","text":"__init__ ( tokenizer : Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ], max_length : Union [ int , None ] = None ) -> None Initializes the dataset chat tokenizer. Parameters: tokenizer ( Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ] ) \u2013 pre-trained transformers tokenizer. max_length ( optional , default: None ) \u2013 maximum sequence length after truncation.","title":"__init__"},{"location":"overview/#lotd.LengthFilter","text":"Filters datasets by max and min length of input_ids.","title":"LengthFilter"},{"location":"overview/#lotd.LengthFilter.__call__","text":"__call__ ( input_ids : List [ List [ int ]]) -> List [ bool ] Processes a batch of input_ids.","title":"__call__"},{"location":"overview/#lotd.LengthFilter.__init__","text":"__init__ ( min_length : int = 0 , max_length : Union [ int , None ] = None ) -> None Initializes the collator. Parameters: min_length ( int , default: 0 ) \u2013 lowest number of tokens. max_length ( Union [ int , None] , default: None ) \u2013 highest number of tokens.","title":"__init__"},{"location":"overview/#lotd.PadCollator","text":"Collator which converts inputs into pytorch tensors and applies padding.","title":"PadCollator"},{"location":"overview/#lotd.PadCollator.__call__","text":"__call__ ( batch : List [ Dict [ str , Any ]]) -> Dict [ str , torch . Tensor ] Collates inputs. Parameters: batch ( List [ Dict [ str , Any ]] ) \u2013 list of dicts where each key is a feature. Returns: Dict [ str , Tensor ] \u2013 a dict of padded tensors of form {\"input_ids\": ..., \"attention_mask\": ..., \"prompt_mask\": ...} .","title":"__call__"},{"location":"overview/#lotd.PadCollator.__init__","text":"__init__ ( pad_id : int , pre : Union [ Callable , None ] = None , post : Union [ Callable , None ] = None , padding_side : Literal [ 'right' , 'left' ] = 'right' ) -> None Initializes the collator. Parameters: pad_id ( int ) \u2013 pad token id used for padding. pre ( Union [ Callable , None] , default: None ) \u2013 callable that accepts list of dicts where every key is a dataset feature, modifies it and returns it back. post ( Union [ Callable , None] , default: None ) \u2013 callable that accepts a dict of tensors of a form {\"input_ids\": ..., \"attention_mask\": ..., \"prompt_mask\": ...} , modifies it and returns it back. padding_side ( Literal ['right', 'left'] , default: 'right' ) \u2013 can be right or left.","title":"__init__"},{"location":"overview/#lotd.TextTokenizer","text":"Applies template and tokenizes texts in a dataset.","title":"TextTokenizer"},{"location":"overview/#lotd.TextTokenizer.__call__","text":"__call__ ( texts : List [ str ]) -> Dict [ str , List [ List [ int ]]] Tokenizes a batch of text samples. Parameters: texts ( List [ str ] ) \u2013 list of strings. Returns: Dict [ str , List [ List [ int ]]] \u2013 dict with input_ids and prompt_mask for all text in a batch.","title":"__call__"},{"location":"overview/#lotd.TextTokenizer.__init__","text":"__init__ ( tokenizer : Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ], template : str = '[CLS]{{text}}[SEP]' , max_length : Union [ int , None ] = None ) -> None Initializes the dataset tokenizer. Parameters: tokenizer ( Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ] ) \u2013 pre-trained transformers tokenizer. template ( str , default: '[CLS]{{text}}[SEP]' ) \u2013 jinja2 template containing {{text}} keyword. max_length ( optional , default: None ) \u2013 maximum sequence length after truncation.","title":"__init__"},{"location":"overview/#lotd.format_chat","text":"format_chat ( prompts : Union [ str , List [ str ]] = [], responses : Union [ str , List [ str ]] = [], system : Union [ str , None ] = None ) -> List [ Dict [ str , str ]] Format list of prompts, responses and system messages into a chat. Parameters: prompts ( Union [ str , List [ str ]] , default: [] ) \u2013 a single prompt or a list of prompts provided by a user in a dialog. responses ( Union [ str , List [ str ]] , default: [] ) \u2013 a single response or a list of responses producrd by a model in a dialog. system ( Union [ str , None] , default: None ) \u2013 system prompt used for this chat. Returns: List [ Dict [ str , str ]] \u2013 a list of messages of form [{\"role\": \"...\", \"content\": \"...\"}, ...] .","title":"format_chat"},{"location":"overview/#lotd.generate_chat_template","text":"generate_chat_template ( last_only : bool = False , wrapper : str = '{{template}}' , system_tags : Tuple [ str , str ] = ( '<|system|>' , '<|end|>' ), user_tags : Tuple [ str , str ] = ( '<|user|>' , '<|end|>' ), assistant_tags : Tuple [ str , str ] = ( '<|assistant|>' , '<|end|>' )) -> str Generate chat template for instruction tuning. Parameters: last_only ( bool , default: False ) \u2013 only enable generation for last response. If true, assistant_masks ignores previous responses. Default: False. wrapper ( str , default: '{{template}}' ) \u2013 a wrapper for adding text before and after the template. Should contain {{template}} . system_tags ( Tuple [ str , str ] , default: ('<|system|>', '<|end|>') ) \u2013 a pair of tags to mark the beginning and the end of a system role in a dialog. user_tags ( Tuple [ str , str ] , default: ('<|user|>', '<|end|>') ) \u2013 see system_tags . assistant_tags ( Tuple [ str , str ] , default: ('<|assistant|>', '<|end|>') ) \u2013 see system_tags . Returns: str \u2013 a jinja2 template string.","title":"generate_chat_template"},{"location":"overview/#lotd.get_loaders","text":"get_loaders ( dataset : Dataset , collate_fn : Callable = lambda x : x , batch_size : int = 16 , train_size : float = 0.8 , val_size : float = 0.1 , num_workers : int = 15 , seed : int = 42 ) -> Tuple [ DataLoader , DataLoader , DataLoader ] Shortcut to generate pytorch dataloaders (train/val/test) from hf dataset. Parameters: dataset ( Dataset ) \u2013 HF dataset. collate_fn ( Callable , default: lambda x: x ) \u2013 function used for dataset collation. batch_size ( int , default: 16 ) \u2013 batch_size for dataloaders. train_size ( float , default: 0.8 ) \u2013 train split size. see split_dataset . val_size ( float , default: 0.1 ) \u2013 validation split size. see split_dataset . num_workers ( int , default: 15 ) \u2013 number of pytorch dataloader workers. seed ( int , default: 42 ) \u2013 random seed used for splitting. Returns: Tuple [ DataLoader , DataLoader , DataLoader ] \u2013 a tuple with train, validation and test pytorch dataloaders. Splits dataset and assigns collators automatically.","title":"get_loaders"},{"location":"overview/#lotd.load_cached","text":"load_cached ( cache_path : str , process_fn : Callable ) -> Dataset Try loading processed dataset from cache. Processes dataset and saves it to cache if pre-cached dataset is not found. Parameters: cache_path ( str ) \u2013 path to load/save dataset. process_fn ( Callable ) \u2013 function that will return a new processed dataset if cache is not found. Returns: Dataset \u2013 a pre-processed HF dataset.","title":"load_cached"},{"location":"overview/#lotd.split_dataset","text":"split_dataset ( dataset : Dataset , train_size : float = 0.8 , val_size : float = 0.1 , seed : int = 42 ) -> Tuple [ Dataset , Dataset , Dataset ] Split HF dataset into train, validation and test. Parameters: dataset ( Dataset ) \u2013 HF dataset. train_size ( float , default: 0.8 ) \u2013 train ratio from 0 to 1. val_size ( float , default: 0.1 ) \u2013 validation ratio from 0 to 1. seed ( int , default: 42 ) \u2013 seed used for splitting. Returns: Tuple [ Dataset , Dataset , Dataset ] \u2013 a tuple of 3 datasets for train, validation and test. train_size and val_size are taken from total and their sum should not be more than 1. Test size would be equal to 1 - train_size - val_size.","title":"split_dataset"},{"location":"overview/#datasets","text":"","title":"Datasets"},{"location":"overview/#lotd.datasets","text":"","title":"datasets"},{"location":"overview/#lotd.datasets.alpaca","text":"alpaca ( tokenizer : Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ], cache_path : Union [ str , None ] = None , max_length : int = 512 , batch_size : int = 16 , seed : int = 42 , pre : Union [ Callable , None ] = None , post : Union [ Callable , None ] = None ) -> Tuple [ DataLoader , DataLoader , DataLoader ] Downloads and Pre-processes Alpaca dataset for instruction fine-tuning. Parameters: tokenizer ( Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ] ) \u2013 transformers tokenizer with chat_template parameter set. cache_path ( Union [ str , None] , default: None ) \u2013 path to load/save dataset. max_length ( int , default: 512 ) \u2013 maximum sequence length for filter. batch_size ( int , default: 16 ) \u2013 dataloaders batch size. seed ( int , default: 42 ) \u2013 splitting and shuffling seed. pre ( Union [ Callable , None] , default: None ) \u2013 see PadCollator . post ( Union [ Callable , None] , default: None ) \u2013 see PadCollator . Returns: Tuple [ DataLoader , DataLoader , DataLoader ] \u2013 a tuple of train, validation and test dataloaders.","title":"alpaca"},{"location":"processors/","text":"Dataset processors and filters Callable objects passed to HF dataset map() and filter() functions. lotd.TextTokenizer Applies template and tokenizes texts in a dataset. __call__ __call__ ( texts : List [ str ]) -> Dict [ str , List [ List [ int ]]] Tokenizes a batch of text samples. Parameters: texts ( List [ str ] ) \u2013 list of strings. Returns: Dict [ str , List [ List [ int ]]] \u2013 dict with input_ids and prompt_mask for all text in a batch. __init__ __init__ ( tokenizer : Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ], template : str = '[CLS]{{text}}[SEP]' , max_length : Union [ int , None ] = None ) -> None Initializes the dataset tokenizer. Parameters: tokenizer ( Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ] ) \u2013 pre-trained transformers tokenizer. template ( str , default: '[CLS]{{text}}[SEP]' ) \u2013 jinja2 template containing {{text}} keyword. max_length ( optional , default: None ) \u2013 maximum sequence length after truncation. lotd.ChatTokenizer Applies chat template and tokenizes messages in a dataset. __call__ __call__ ( prompts : Union [ List [ str ], List [ List [ str ]]], responses : Union [ List [ str ], List [ List [ str ]]] = [], system : Union [ List [ str ], None ] = None ) -> Dict [ str , List [ List [ int ]]] Tokenizes chats. Parameters: prompts ( Union [ List [ str ], List [ List [ str ]]] ) \u2013 a list where each item is either a single prompt or multiple prompts in a dialog. responses ( Union [ List [ str ], List [ List [ str ]]] , default: [] ) \u2013 a list where each item is either a single response or multiple responses in a dialog. Returns: Dict [ str , List [ List [ int ]]] \u2013 dict with input_ids and prompt_mask for all text in a batch. __init__ __init__ ( tokenizer : Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ], max_length : Union [ int , None ] = None ) -> None Initializes the dataset chat tokenizer. Parameters: tokenizer ( Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ] ) \u2013 pre-trained transformers tokenizer. max_length ( optional , default: None ) \u2013 maximum sequence length after truncation. lotd.LengthFilter Filters datasets by max and min length of input_ids. __call__ __call__ ( input_ids : List [ List [ int ]]) -> List [ bool ] Processes a batch of input_ids. __init__ __init__ ( min_length : int = 0 , max_length : Union [ int , None ] = None ) -> None Initializes the collator. Parameters: min_length ( int , default: 0 ) \u2013 lowest number of tokens. max_length ( Union [ int , None] , default: None ) \u2013 highest number of tokens.","title":"Processors and Filters"},{"location":"processors/#dataset-processors-and-filters","text":"Callable objects passed to HF dataset map() and filter() functions.","title":"Dataset processors and filters"},{"location":"processors/#lotd.TextTokenizer","text":"Applies template and tokenizes texts in a dataset.","title":"TextTokenizer"},{"location":"processors/#lotd.TextTokenizer.__call__","text":"__call__ ( texts : List [ str ]) -> Dict [ str , List [ List [ int ]]] Tokenizes a batch of text samples. Parameters: texts ( List [ str ] ) \u2013 list of strings. Returns: Dict [ str , List [ List [ int ]]] \u2013 dict with input_ids and prompt_mask for all text in a batch.","title":"__call__"},{"location":"processors/#lotd.TextTokenizer.__init__","text":"__init__ ( tokenizer : Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ], template : str = '[CLS]{{text}}[SEP]' , max_length : Union [ int , None ] = None ) -> None Initializes the dataset tokenizer. Parameters: tokenizer ( Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ] ) \u2013 pre-trained transformers tokenizer. template ( str , default: '[CLS]{{text}}[SEP]' ) \u2013 jinja2 template containing {{text}} keyword. max_length ( optional , default: None ) \u2013 maximum sequence length after truncation.","title":"__init__"},{"location":"processors/#lotd.ChatTokenizer","text":"Applies chat template and tokenizes messages in a dataset.","title":"ChatTokenizer"},{"location":"processors/#lotd.ChatTokenizer.__call__","text":"__call__ ( prompts : Union [ List [ str ], List [ List [ str ]]], responses : Union [ List [ str ], List [ List [ str ]]] = [], system : Union [ List [ str ], None ] = None ) -> Dict [ str , List [ List [ int ]]] Tokenizes chats. Parameters: prompts ( Union [ List [ str ], List [ List [ str ]]] ) \u2013 a list where each item is either a single prompt or multiple prompts in a dialog. responses ( Union [ List [ str ], List [ List [ str ]]] , default: [] ) \u2013 a list where each item is either a single response or multiple responses in a dialog. Returns: Dict [ str , List [ List [ int ]]] \u2013 dict with input_ids and prompt_mask for all text in a batch.","title":"__call__"},{"location":"processors/#lotd.ChatTokenizer.__init__","text":"__init__ ( tokenizer : Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ], max_length : Union [ int , None ] = None ) -> None Initializes the dataset chat tokenizer. Parameters: tokenizer ( Union [ PreTrainedTokenizer , PreTrainedTokenizerFast ] ) \u2013 pre-trained transformers tokenizer. max_length ( optional , default: None ) \u2013 maximum sequence length after truncation.","title":"__init__"},{"location":"processors/#lotd.LengthFilter","text":"Filters datasets by max and min length of input_ids.","title":"LengthFilter"},{"location":"processors/#lotd.LengthFilter.__call__","text":"__call__ ( input_ids : List [ List [ int ]]) -> List [ bool ] Processes a batch of input_ids.","title":"__call__"},{"location":"processors/#lotd.LengthFilter.__init__","text":"__init__ ( min_length : int = 0 , max_length : Union [ int , None ] = None ) -> None Initializes the collator. Parameters: min_length ( int , default: 0 ) \u2013 lowest number of tokens. max_length ( Union [ int , None] , default: None ) \u2013 highest number of tokens.","title":"__init__"},{"location":"templates/","text":"Templates Functions for generating context template or converting inputs ito some specific format lotd.generate_chat_template generate_chat_template ( last_only : bool = False , wrapper : str = '{{template}}' , system_tags : Tuple [ str , str ] = ( '<|system|>' , '<|end|>' ), user_tags : Tuple [ str , str ] = ( '<|user|>' , '<|end|>' ), assistant_tags : Tuple [ str , str ] = ( '<|assistant|>' , '<|end|>' )) -> str Generate chat template for instruction tuning. Parameters: last_only ( bool , default: False ) \u2013 only enable generation for last response. If true, assistant_masks ignores previous responses. Default: False. wrapper ( str , default: '{{template}}' ) \u2013 a wrapper for adding text before and after the template. Should contain {{template}} . system_tags ( Tuple [ str , str ] , default: ('<|system|>', '<|end|>') ) \u2013 a pair of tags to mark the beginning and the end of a system role in a dialog. user_tags ( Tuple [ str , str ] , default: ('<|user|>', '<|end|>') ) \u2013 see system_tags . assistant_tags ( Tuple [ str , str ] , default: ('<|assistant|>', '<|end|>') ) \u2013 see system_tags . Returns: str \u2013 a jinja2 template string. lotd.format_chat format_chat ( prompts : Union [ str , List [ str ]] = [], responses : Union [ str , List [ str ]] = [], system : Union [ str , None ] = None ) -> List [ Dict [ str , str ]] Format list of prompts, responses and system messages into a chat. Parameters: prompts ( Union [ str , List [ str ]] , default: [] ) \u2013 a single prompt or a list of prompts provided by a user in a dialog. responses ( Union [ str , List [ str ]] , default: [] ) \u2013 a single response or a list of responses producrd by a model in a dialog. system ( Union [ str , None] , default: None ) \u2013 system prompt used for this chat. Returns: List [ Dict [ str , str ]] \u2013 a list of messages of form [{\"role\": \"...\", \"content\": \"...\"}, ...] .","title":"Templates"},{"location":"templates/#templates","text":"Functions for generating context template or converting inputs ito some specific format","title":"Templates"},{"location":"templates/#lotd.generate_chat_template","text":"generate_chat_template ( last_only : bool = False , wrapper : str = '{{template}}' , system_tags : Tuple [ str , str ] = ( '<|system|>' , '<|end|>' ), user_tags : Tuple [ str , str ] = ( '<|user|>' , '<|end|>' ), assistant_tags : Tuple [ str , str ] = ( '<|assistant|>' , '<|end|>' )) -> str Generate chat template for instruction tuning. Parameters: last_only ( bool , default: False ) \u2013 only enable generation for last response. If true, assistant_masks ignores previous responses. Default: False. wrapper ( str , default: '{{template}}' ) \u2013 a wrapper for adding text before and after the template. Should contain {{template}} . system_tags ( Tuple [ str , str ] , default: ('<|system|>', '<|end|>') ) \u2013 a pair of tags to mark the beginning and the end of a system role in a dialog. user_tags ( Tuple [ str , str ] , default: ('<|user|>', '<|end|>') ) \u2013 see system_tags . assistant_tags ( Tuple [ str , str ] , default: ('<|assistant|>', '<|end|>') ) \u2013 see system_tags . Returns: str \u2013 a jinja2 template string.","title":"generate_chat_template"},{"location":"templates/#lotd.format_chat","text":"format_chat ( prompts : Union [ str , List [ str ]] = [], responses : Union [ str , List [ str ]] = [], system : Union [ str , None ] = None ) -> List [ Dict [ str , str ]] Format list of prompts, responses and system messages into a chat. Parameters: prompts ( Union [ str , List [ str ]] , default: [] ) \u2013 a single prompt or a list of prompts provided by a user in a dialog. responses ( Union [ str , List [ str ]] , default: [] ) \u2013 a single response or a list of responses producrd by a model in a dialog. system ( Union [ str , None] , default: None ) \u2013 system prompt used for this chat. Returns: List [ Dict [ str , str ]] \u2013 a list of messages of form [{\"role\": \"...\", \"content\": \"...\"}, ...] .","title":"format_chat"},{"location":"utils/","text":"Utils A set of helper functions for processing HF datasets. lotd.load_cached load_cached ( cache_path : str , process_fn : Callable ) -> Dataset Try loading processed dataset from cache. Processes dataset and saves it to cache if pre-cached dataset is not found. Parameters: cache_path ( str ) \u2013 path to load/save dataset. process_fn ( Callable ) \u2013 function that will return a new processed dataset if cache is not found. Returns: Dataset \u2013 a pre-processed HF dataset. lotd.split_dataset split_dataset ( dataset : Dataset , train_size : float = 0.8 , val_size : float = 0.1 , seed : int = 42 ) -> Tuple [ Dataset , Dataset , Dataset ] Split HF dataset into train, validation and test. Parameters: dataset ( Dataset ) \u2013 HF dataset. train_size ( float , default: 0.8 ) \u2013 train ratio from 0 to 1. val_size ( float , default: 0.1 ) \u2013 validation ratio from 0 to 1. seed ( int , default: 42 ) \u2013 seed used for splitting. Returns: Tuple [ Dataset , Dataset , Dataset ] \u2013 a tuple of 3 datasets for train, validation and test. train_size and val_size are taken from total and their sum should not be more than 1. Test size would be equal to 1 - train_size - val_size. lotd.get_loaders get_loaders ( dataset : Dataset , collate_fn : Callable = lambda x : x , batch_size : int = 16 , train_size : float = 0.8 , val_size : float = 0.1 , num_workers : int = 15 , seed : int = 42 ) -> Tuple [ DataLoader , DataLoader , DataLoader ] Shortcut to generate pytorch dataloaders (train/val/test) from hf dataset. Parameters: dataset ( Dataset ) \u2013 HF dataset. collate_fn ( Callable , default: lambda x: x ) \u2013 function used for dataset collation. batch_size ( int , default: 16 ) \u2013 batch_size for dataloaders. train_size ( float , default: 0.8 ) \u2013 train split size. see split_dataset . val_size ( float , default: 0.1 ) \u2013 validation split size. see split_dataset . num_workers ( int , default: 15 ) \u2013 number of pytorch dataloader workers. seed ( int , default: 42 ) \u2013 random seed used for splitting. Returns: Tuple [ DataLoader , DataLoader , DataLoader ] \u2013 a tuple with train, validation and test pytorch dataloaders. Splits dataset and assigns collators automatically.","title":"Utils"},{"location":"utils/#utils","text":"A set of helper functions for processing HF datasets.","title":"Utils"},{"location":"utils/#lotd.load_cached","text":"load_cached ( cache_path : str , process_fn : Callable ) -> Dataset Try loading processed dataset from cache. Processes dataset and saves it to cache if pre-cached dataset is not found. Parameters: cache_path ( str ) \u2013 path to load/save dataset. process_fn ( Callable ) \u2013 function that will return a new processed dataset if cache is not found. Returns: Dataset \u2013 a pre-processed HF dataset.","title":"load_cached"},{"location":"utils/#lotd.split_dataset","text":"split_dataset ( dataset : Dataset , train_size : float = 0.8 , val_size : float = 0.1 , seed : int = 42 ) -> Tuple [ Dataset , Dataset , Dataset ] Split HF dataset into train, validation and test. Parameters: dataset ( Dataset ) \u2013 HF dataset. train_size ( float , default: 0.8 ) \u2013 train ratio from 0 to 1. val_size ( float , default: 0.1 ) \u2013 validation ratio from 0 to 1. seed ( int , default: 42 ) \u2013 seed used for splitting. Returns: Tuple [ Dataset , Dataset , Dataset ] \u2013 a tuple of 3 datasets for train, validation and test. train_size and val_size are taken from total and their sum should not be more than 1. Test size would be equal to 1 - train_size - val_size.","title":"split_dataset"},{"location":"utils/#lotd.get_loaders","text":"get_loaders ( dataset : Dataset , collate_fn : Callable = lambda x : x , batch_size : int = 16 , train_size : float = 0.8 , val_size : float = 0.1 , num_workers : int = 15 , seed : int = 42 ) -> Tuple [ DataLoader , DataLoader , DataLoader ] Shortcut to generate pytorch dataloaders (train/val/test) from hf dataset. Parameters: dataset ( Dataset ) \u2013 HF dataset. collate_fn ( Callable , default: lambda x: x ) \u2013 function used for dataset collation. batch_size ( int , default: 16 ) \u2013 batch_size for dataloaders. train_size ( float , default: 0.8 ) \u2013 train split size. see split_dataset . val_size ( float , default: 0.1 ) \u2013 validation split size. see split_dataset . num_workers ( int , default: 15 ) \u2013 number of pytorch dataloader workers. seed ( int , default: 42 ) \u2013 random seed used for splitting. Returns: Tuple [ DataLoader , DataLoader , DataLoader ] \u2013 a tuple with train, validation and test pytorch dataloaders. Splits dataset and assigns collators automatically.","title":"get_loaders"}]}